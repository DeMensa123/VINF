{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import re\n",
    "import json\n",
    "import pysolr\n",
    "import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "import pyspark.sql\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import SQLContext\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName(\"FreebasePeople\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_file = \"freebase-head-1000000\"\n",
    "# input_file = \"freebase-head-10000000\"\n",
    "input_file = \"freebase-head-100000000\"\n",
    "freebase = spark.sparkContext.textFile(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_name = '(\\/[gm]\\..+\\t<http:\\/\\/rdf\\.freebase\\.com\\/ns\\/type\\.object\\.name>\\t\\\".*\\\"@en)'\n",
    "re_alias = '(\\/[gm]\\..+\\t<http:\\/\\/rdf\\.freebase\\.com\\/ns\\/common\\.topic\\.alias>\\t\\\".*\\\"@en)'\n",
    "re_birth = '(\\/[gm]\\..+\\t<http:\\/\\/rdf\\.freebase\\.com\\/ns\\/people\\.person\\.date_of_birth>\\t)'\n",
    "re_death = '(\\/[gm]\\..+\\t<http:\\/\\/rdf\\.freebase\\.com\\/ns\\/people\\.deceased_person\\.date_of_death>\\t)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = freebase \\\n",
    "    .filter(lambda x: re.search(re_name,x) or re.search(re_alias,x) or re.search(re_birth,x) or re.search(re_death,x)) \\\n",
    "    .distinct() \\\n",
    "    .map(lambda x: re.sub('(http\\:\\/\\/rdf.freebase.com\\/ns\\/)|(\\^\\^.*\\.)|(\\@.*\\.)|\\<|\\>|\\\"',\"\",x)) \\\n",
    "    .map(lambda x: x.split('\\t')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# people.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('subject', StringType(), True),\n",
    "                     StructField('predicate', StringType(), True),\n",
    "                     StructField('object', StringType(), True, metadata = {\"maxlength\":2048})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = spark.createDataFrame(people.filter(lambda x: \"type.object.name\" in x[1]), schema)\n",
    "aliases = spark.createDataFrame(people.filter(lambda x: \"common.topic.alias\" in x[1]), schema)\n",
    "births = spark.createDataFrame(people.filter(lambda x: \"people.person.date_of_birth\" in x[1]), schema)\n",
    "deaths = spark.createDataFrame(people.filter(lambda x: \"people.deceased_person.date_of_death\" in x[1]), schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pomenovanie tabuliek\n",
    "names.registerTempTable(\"names\")\n",
    "aliases.registerTempTable(\"aliases\")\n",
    "births.registerTempTable(\"births\")\n",
    "deaths.registerTempTable(\"deaths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_context = SQLContext(spark.sparkContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = sql_context.sql(\"\"\"\n",
    "select names.object as name, \n",
    "ifnull(aliases.object, '-') as alias,\n",
    "ifnull(cast(births.object as date), (cast(deaths.object as date) - 100*365) ) as birth,\n",
    "ifnull(cast(deaths.object as date), (cast(births.object as date) + 100*365) ) as death\n",
    "from names\n",
    "left join births on names.subject = births.subject\n",
    "left join deaths on names.subject = deaths.subject\n",
    "left join aliases on names.subject = aliases.subject\n",
    "where births.object is not null or deaths.object is not null\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 10\n",
    "# n = 100\n",
    "n = 1000\n",
    "# result_files = \"filteredPeople_1\"\n",
    "# result_files = \"filteredPeople_10\"\n",
    "result_files = \"filteredPeople_100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql.repartition(n).write.format('com.databricks.spark.csv') \\\n",
    "    .option(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\",\"false\") \\\n",
    "    .save(result_files, header = 'true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import to_json, spark_partition_id, collect_list, col, struct\n",
    "\n",
    "# sql.select(to_json(struct(*sql.columns)).alias(\"json\"))\\\n",
    "#     .groupBy(spark_partition_id())\\\n",
    "#     .agg(collect_list(\"json\").alias(\"json_list\"))\\\n",
    "#     .select(col(\"json_list\").cast(\"string\"))\\\n",
    "#     .write.json(\"path_json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "\n",
    "readPath = result_files\n",
    "writePath = result_files + '_csv'\n",
    "os.mkdir(writePath)\n",
    "\n",
    "file_list = [f for f in listdir(readPath)]\n",
    "\n",
    "for i in file_list:\n",
    "    print(i)\n",
    "    filename, file_extension = os.path.splitext(i)\n",
    "    reg = '(\\.[\\w]+-[\\w]+)|([\\w]+-[\\w]+)'\n",
    "    result = re.match(reg, filename).group()\n",
    "    if file_extension == '.csv':\n",
    "        filename = filename.split('-')[1]\n",
    "        os.rename(readPath + '/' + i, writePath + \"/\" + result + file_extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whoosh.index import *\n",
    "from whoosh.fields import *\n",
    "from whoosh.analysis import *\n",
    "from whoosh.support.charset import accent_map\n",
    "import os\n",
    "import csv\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './filteredPeople_1_csv/part-00000.csv'\n",
    "csv_file = open('goods.csv', 'r')  # csv file\n",
    "data = csv.reader(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = Schema(name = TEXT(stored=True),alias = TEXT(stored=True),birth = TEXT(stored=True),death = TEXT(stored=True))\n",
    "\n",
    "if not os.path.exists(\"index\"):\n",
    "    os.mkdir(\"index\")\n",
    "ix = create_in(\"index\", schema)\n",
    "\n",
    "ix = open_dir(\"index\")\n",
    "\n",
    "# writer = ix.writer()\n",
    "csv_file = open(path, 'r', encoding = 'utf8')  # csv file\n",
    "data = csv.reader(csv_file)\n",
    "for i in data:\n",
    "    print(writer.add_document(\n",
    "        name=i[0],\n",
    "        alias=i[1],\n",
    "        birth = i[2],\n",
    "        death = i[3]\n",
    "    ))\n",
    "        \n",
    "# for i in file_list:\n",
    "# with open(path, 'r', encoding='utf-8') as file:\n",
    "#     for line in file:\n",
    "        \n",
    "#         print(writer.add_document(content=line))\n",
    "\n",
    "# writer.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whoosh.qparser import QueryParser\n",
    "qp = QueryParser(\"name\", schema = ix.schema)\n",
    "q = qp.parse(\"Mayu Fukunoue\")\n",
    "print(q)\n",
    "with ix.searcher() as s:\n",
    "    results = s.search(q)\n",
    "    print(s.search(q))\n",
    "    for res in results:\n",
    "        print(res)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysolr = pysolr.Solr('http://localhost:8983/solr/freebase_people_100/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mysolr.delete(q='*', commit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mysolr.add(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_name_1 = 'Takahama'\n",
    "person_name_2 = 'Louise'\n",
    "query_1 = 'name:*' + person_name_1 + '*'\n",
    "query_2 = 'name:*' + person_name_2 + '*'\n",
    "# print(q)\n",
    "result_1 = mysolr.search(query_1)\n",
    "result_2 = mysolr.search(query_2)\n",
    "list(result_1), list(result_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in result_1:\n",
    "    person_1, alias_1, dob_1, dod_1 = str(line['name'][0]), str(line['alias'][0]), line['birth'][0], line['death'][0]\n",
    "    print(person_1, alias_1, dob_1, dod_1)\n",
    "    \n",
    "for line in result_2:\n",
    "    person_2, alias_2, dob_2, dod_2 = str(line['name'][0]), str(line['alias'][0]), line['birth'][0], line['death'][0]\n",
    "    print(person_2, alias_2, dob_2, dod_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDifference(dob_1, dod_1, dob_2, dod_2):\n",
    "    if dod_1 >= dob_2 and dob_1 <= dod_2:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meet = getDifference(dob_1, dod_1, dob_2, dod_2)\n",
    "if meet:\n",
    "    print(\"Osoby\" + person_1 +\" a \" + person_2 + \" sa mohli stretnut.\")\n",
    "else:\n",
    "    print(\"Osoby\" + person_1 +\" a \" + person_2 + \" sa nemohli stretnut.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
